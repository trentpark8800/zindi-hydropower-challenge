{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9e93af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import duckdb as ddb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc284df",
   "metadata": {},
   "source": [
    "## Data Extraction and Aggregation\n",
    "\n",
    "Due to the volume of data found in `Data.csv` I found it easier to operate on the data using DuckDB. At a high level I wanted a location from which I could easily extract data as and when I needed, as well as store results without flooding my RAM.\n",
    "\n",
    "The steps taken are:\n",
    "- Creating a DuckDB instance\n",
    "- Importing data from the `Data.csv`, `Kalam Climate Data.xlsx` and `SampleSubmission.csv` files\n",
    "- Aggregate the raw data (hydropower data and climate data) into a daily granularity tables\n",
    "- Use the database as and when needed to store data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3894666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the database and two schemas\n",
    "con = ddb.connect(\"./kalam_hydropower.db\")\n",
    "\n",
    "try:\n",
    "    con.sql(\"create schema 'raw';\")\n",
    "    con.sql(\"create schema 'prepared';\")\n",
    "except ddb.CatalogException as e:\n",
    "    print(f\"Schemas already exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bd86f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the datatypes etc before loading - note that v_blue and v_yellow are detected as varchars and must be explicitly cast to doubles\n",
    "con.sql(\"select * from read_csv('./data/Data.csv') limit 10;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b9fc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table ignoring the columns consumer device 9 etc - these are assumed to be errors\n",
    "try:\n",
    "    con.sql(\"\"\"\n",
    "        create table raw.hydropower_production as\n",
    "            select date_time, Source as source, v_red, cast(v_blue as double) as v_blue, cast(v_yellow as double) as v_yellow, current, kwh\n",
    "            from read_csv('./data/Data.csv');\n",
    "    \"\"\")\n",
    "except ddb.CatalogException as e:\n",
    "    print(f\"Table already exists: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c3f2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a package needed to read excel natively in DDB\n",
    "con.sql(\"INSTALL excel; LOAD excel\")\n",
    "\n",
    "con.sql(\"select * from './data/Climate Data/Kalam Climate Data.xlsx' limit 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dd2ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename climate column names as the unusual characters make things difficult\n",
    "try:\n",
    "    con.sql(\"\"\"\n",
    "        create table raw.climate as\n",
    "            select \"Date Time\" as date_time, \"Temperature (°C)\" as temperature, \"Dewpoint Temperature (°C)\" as dewpoint_temperature, \"U Wind Component (m/s)\" as u_wind_component, \"V Wind Component (m/s)\" as v_wind_component, \"Total Precipitation (mm)\" as total_precipitation, \"Snowfall (mm)\" as snowfall, \"Snow Cover (%)\" as snow_cover_perc\n",
    "            from './data/Climate Data/Kalam Climate Data.xlsx'\n",
    "    \"\"\")\n",
    "except ddb.CatalogException as e:\n",
    "    print(f\"Table already exists: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7204e89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.sql(\"select * from raw.climate limit 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915dbb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    con.sql(\"\"\"\n",
    "        create table raw.sample_submission as\n",
    "            select * from read_csv('./data/SampleSubmission.csv')\n",
    "    \"\"\")\n",
    "except ddb.CatalogException as e:\n",
    "    print(f\"Table already exists: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eab8685",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.sql(\"select * from raw.sample_submission limit 10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b4ce3f",
   "metadata": {},
   "source": [
    "### Aggregation\n",
    "The main goal here is to make the tables `raw.hydropower_production` and `raw.climate` exist at a daily granularity, this is easier to work with in pandas and the forecasting exercise/validation occurs on a daily level rather than at 5 minute or 1 hour intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a6c6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the power production data\n",
    "try:\n",
    "    con.sql(\"\"\"\n",
    "    create table prepared.daily_hydropower_production as\n",
    "            with temp as (\n",
    "                select\n",
    "                    *,\n",
    "                    cast(date_time as date) as date,\n",
    "                    regexp_extract(source, 'consumer_device_(\\d+)', 1) as consumer_device,\n",
    "                    regexp_extract(source, '_data_user_(\\d+)', 1) as data_user\n",
    "                from raw.hydropower_production\n",
    "            )\n",
    "            select\n",
    "                date,\n",
    "                source,\n",
    "                consumer_device,\n",
    "                data_user,\n",
    "                sum(kwh) as kwh\n",
    "            from temp\n",
    "            group by date, source, consumer_device, data_user\n",
    "            order by source, date\n",
    "    \"\"\")\n",
    "except ddb.CatalogException as e:\n",
    "    print(f\"Table already exists: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add4cf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.sql(\"select * from prepared.daily_hydropower_production limit 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35745b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly aggregate the climate data\n",
    "try:\n",
    "    con.sql(\"\"\"\n",
    "        create table prepared.daily_climate as (\n",
    "        select\n",
    "            cast(date_time as date) as date,\n",
    "            avg(temperature) as avg_temperature,\n",
    "            avg(dewpoint_temperature) as avg_dewpoint_temperature,\n",
    "            avg(u_wind_component) as avg_u_wind_component,\n",
    "            avg(v_wind_component) as avg_v_wind_component,\n",
    "            sum(total_precipitation) as total_precipitation,\n",
    "            sum(snowfall) as total_snowfall,\n",
    "            avg(snow_cover_perc) as avg_snow_cover_perc\n",
    "        from raw.climate\n",
    "        group by cast(date_time as date)\n",
    "    )\n",
    "    \"\"\")\n",
    "except ddb.CatalogException as e:\n",
    "    print(f\"Table already exists: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b41339",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.sql(\"select * from prepared.daily_climate limit 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f46003",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
