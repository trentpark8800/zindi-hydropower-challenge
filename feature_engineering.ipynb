{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9e93af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import duckdb as ddb\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b239311",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = ddb.connect(\"./kalam_hydropower.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c54b5a3",
   "metadata": {},
   "source": [
    "## Analysis and Feature Engineering\n",
    "Now that the data has been aggregated, it is easier to work with in pandas and visualise using matplotlib and seaborn. The inention of this section is to:\n",
    "- Produce features that are intuitive - i.e. hypothesize on what could support forecasting\n",
    "- Visualise the feature signals to understand how the data varies with the target signal over time\n",
    "- Check the feature covariation with the target data (daily kwh) to get a sense of which features to try\n",
    "\n",
    "We start by examining the high-level features of the daily hydropower production data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e56905",
   "metadata": {},
   "outputs": [],
   "source": [
    "dhp_df = con.sql(\"\"\"\n",
    "    select\n",
    "        *\n",
    "    from prepared.daily_hydropower_production\n",
    "\"\"\").to_df()\n",
    "\n",
    "dhp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2304dbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dhp_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9406450",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_dhp_df = dhp_df[[\"date\", \"kwh\"]].groupby(\"date\", as_index=False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab2283e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_dhp_df.plot(x=\"date\", y=\"kwh\", figsize=(18, 5), title=\"Daily Power Production\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e928ffa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_df = con.sql(\"select * from raw.sample_submission\").to_df()\n",
    "\n",
    "ss_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93c80ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_df[[\"date\", \"source\"]] = ss_df['ID'].str.split('_', n=1, expand=True)\n",
    "\n",
    "ss_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643e106f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of unique sources in actual data: {len(dhp_df['source'].unique())}\")\n",
    "print(f\"Number of unique sources in submission data: {len(ss_df['source'].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6463f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are devices with weak signals which we can exclude from the analysis - these were mentioned in the samplesubmission notebook\n",
    "devices_to_drop = [\"3\", \"5\", \"11\", \"14\", \"15\", \"17\", \"24\", \"25\", \"27\", \"33\", \"4\", \"9\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40129913",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_forecast_df = dhp_df[~(dhp_df[\"consumer_device\"].isin(devices_to_drop))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68388e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the data such that all the different device, user combinations become independent columns at the same length\n",
    "to_forecast_pivotted_df = to_forecast_df.pivot(columns=\"source\", index=\"date\", values=\"kwh\").copy()\n",
    "\n",
    "# Not all of the signals have the same length, in this case I simply fill missing/non-existant values with a 0\n",
    "to_forecast_pivotted_df.fillna(0, inplace=True)\n",
    "\n",
    "to_forecast_pivotted_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77e5ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I melt the dataframe back into a row-based structure, but I am guarenteed that all signals are the same length\n",
    "to_forecast_extended_df = to_forecast_pivotted_df.melt(value_vars=list(to_forecast_pivotted_df.columns), var_name=\"source\", value_name=\"kwh\", ignore_index=False)\n",
    "\n",
    "to_forecast_extended_df.reset_index(inplace=True, drop=False)\n",
    "\n",
    "to_forecast_extended_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4292a4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I create a dataframe with the sum of kwh across all user/device combinations for easy visualisation (i.e. one series instead of over 400)\n",
    "power_df = to_forecast_extended_df[[\"date\", \"kwh\"]].groupby(\"date\", as_index=False).sum()\n",
    "\n",
    "power_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a489abe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I pull the daily climate data into a dataframe for feature engineering\n",
    "climate_df = con.sql(\"select * from prepared.daily_climate\").to_df()\n",
    "\n",
    "climate_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00af15f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features taken from an example notebook - these seem intuitive as power production likely depends on rainfall and temperature (melting of snow)\n",
    "climate_df[\"temp_dew_diff\"] = climate_df[\"avg_temperature\"] - climate_df[\"avg_dewpoint_temperature\"]\n",
    "climate_df[\"wind_speed\"] = (climate_df[\"avg_u_wind_component\"]**2 + climate_df[\"avg_v_wind_component\"]**2)**0.5\n",
    "climate_df[\"precip_snow_ratio\"] = climate_df[\"total_precipitation\"] / (climate_df[\"total_snowfall\"] + 1e-6)\n",
    "climate_df[\"precip_minus_snow\"] = climate_df[\"total_precipitation\"] - climate_df[\"total_snowfall\"]\n",
    "\n",
    "# Some more 'out there' features to try - these were basically some brainstorm features I thought might work\n",
    "climate_df[\"rolling_precip_7d\"] = climate_df[\"total_precipitation\"].rolling(7).sum()\n",
    "climate_df[\"rolling_snow_30d\"] = climate_df[\"total_snowfall\"].rolling(30).sum()\n",
    "climate_df[\"melt_potential\"] = climate_df[\"avg_temperature\"] * climate_df[\"avg_snow_cover_perc\"]\n",
    "climate_df[\"wind_variability_3d\"] = climate_df[\"wind_speed\"].rolling(3).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73a5939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal features - features based on time/seasonality\n",
    "climate_df[\"dayofyear\"] = climate_df[\"date\"].dt.dayofyear\n",
    "climate_df[\"month\"] = climate_df[\"date\"].dt.month\n",
    "climate_df[\"sin_doy\"] = np.sin(2 * np.pi * climate_df[\"dayofyear\"] / 365)\n",
    "climate_df[\"cos_doy\"] = np.cos(2 * np.pi * climate_df[\"dayofyear\"] / 365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f6cb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_df = pd.merge(climate_df, power_df, on=\"date\", how=\"left\")\n",
    "\n",
    "all_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbe285b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The features and the target data are all on different scales, and I want to observe the shape of the signals,\n",
    "# so for this I produce a scaled set of data to visualise\n",
    "scaler = StandardScaler()\n",
    "\n",
    "numeric_columns = all_data_df.select_dtypes(\"number\").columns\n",
    "\n",
    "scaled_data_df = all_data_df.copy()\n",
    "scaled_data_df[numeric_columns] = scaler.fit_transform(all_data_df[numeric_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd76a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I define the feature columns I want to plot against kwh\n",
    "feature_columns = list(all_data_df.columns)\n",
    "\n",
    "feature_columns.remove(\"date\")\n",
    "feature_columns.remove(\"kwh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac37751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the kWh components and feature components\n",
    "kwh_components = ['kwh']\n",
    "features = feature_columns\n",
    "\n",
    "# Create subplots: one row per feature\n",
    "fig, axes = plt.subplots(len(features), len(kwh_components), figsize=(18, 50), sharex=True)\n",
    "fig.suptitle('Comparison of kWh Components with Weather Features (Scaled)', fontsize=16)\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    ax = axes[i]\n",
    "    ax.plot(scaled_data_df['date'], scaled_data_df[feature], label=feature)\n",
    "    ax.plot(scaled_data_df['date'], scaled_data_df[\"kwh\"], label=\"kwh\", linestyle='--')\n",
    "    ax.set_ylabel(feature)\n",
    "    if i == len(features) - 1:\n",
    "        ax.set_xlabel('Date')\n",
    "    if i == 0:\n",
    "        ax.set_title(\"Total kwh\")\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dc71c3",
   "metadata": {},
   "source": [
    "What is clear from these sub-plots is the following:\n",
    "- There is a significant 'downtime' period from 2024-01 to 2024-07, no power is being produced between those times\n",
    "- Precipitation minus snow (only rainfall) coincides with power production\n",
    "- Snowfall does not seem to have an impact in 2024 - i.e. high snowfall does not imply high power production\n",
    "- Temperature rises in 2024 seem to coincide with power production\n",
    "- There is clearly a seasonality in power production, and temporal features reflect that, sin_doy, dayofyear and month seem to be good candidates to reflect this\n",
    "\n",
    "Now, given that this is a time series problem it may not be that the date-aligned features at some lag may be better predictors. I want to understand correlation over various lags of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5b31ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the data is ordered appropriately for the correlation check\n",
    "all_data_df.sort_values(by=\"date\", inplace=True)\n",
    "all_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33a4b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define lag periods (in days)\n",
    "lags = {\n",
    "    '0_lag': 0,\n",
    "    '1_day': 1,\n",
    "    '1_week': 7,\n",
    "    '1_month': 30,\n",
    "    '2_month': 60,\n",
    "    '1_quarter': 90\n",
    "}\n",
    "\n",
    "# Store correlations for each lag\n",
    "lagged_corrs = pd.DataFrame(index=features)\n",
    "\n",
    "for label, lag_days in lags.items():\n",
    "    df_lagged = all_data_df[all_data_df[\"date\"] >= pd.Timestamp(\"2024-01-01\")].copy()\n",
    "    for feature in features:\n",
    "        df_lagged[feature + '_lag'] = df_lagged[feature].shift(lag_days)\n",
    "    df_lagged = df_lagged.dropna(subset=[f + '_lag' for f in features])\n",
    "    \n",
    "    corr_series = df_lagged[[f + '_lag' for f in features] + ['kwh']].corr()['kwh']\n",
    "    # Drop 'kwh' itself and rename the index to remove '_lag'\n",
    "    corr_series = corr_series.drop('kwh')\n",
    "    corr_series.index = [i.replace('_lag', '') for i in corr_series.index]\n",
    "    \n",
    "    lagged_corrs[label] = corr_series\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.heatmap(lagged_corrs, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Lagged Correlation of Weather Features with avg_kwh')\n",
    "plt.ylabel('Feature')\n",
    "plt.xlabel('Lag Period')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e31c10",
   "metadata": {},
   "source": [
    "The following features at a lag of 30 days (~1month) seem to be good candidates\n",
    "- Temperature/dewpoint temperature\n",
    "- Wind speed\n",
    "- Precip minus snow (rainfall)\n",
    "- Precip to snow ratio\n",
    "\n",
    "Now some of the features have strong correlations (like those involving snow) but looking at the signal plots, these could be spurrious relationships and don't make intuitive sense to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9841ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The features are stored for future use\n",
    "try:\n",
    "    con.sql(\"\"\"\n",
    "        create table prepared.daily_features as\n",
    "            select * from climate_df\n",
    "    \"\"\")\n",
    "except ddb.CatalogException as e:\n",
    "    print(f\"Table already exists: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00088b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
