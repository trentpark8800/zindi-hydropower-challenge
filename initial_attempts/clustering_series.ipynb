{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaa644f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import duckdb as ddb\n",
    "from tsfresh import extract_features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25281616",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = ddb.connect(\"../kalam_hydropower.db\", read_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22dfd91",
   "metadata": {},
   "source": [
    "## Clustering Series Together\n",
    "\n",
    "The idea here was to try reduce the number of series I was dealing with by introducing groups of 'similar' series together. Now this ultimately did not work out for me, however, I think this is valueable because of the use of `tsfresh` and the ease of extracting high level information from a large set of timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443ad4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_df = con.sql(\"select * from raw.sample_submission\").to_df()\n",
    "ss_df[[\"date\", \"source\"]] = ss_df[\"ID\"].str.split(\"_\", n=1, expand=True)\n",
    "\n",
    "\n",
    "ss_df = con.sql(\"select * from raw.sample_submission\").to_df()\n",
    "ss_df[[\"date\", \"source\"]] = ss_df[\"ID\"].str.split(\"_\", n=1, expand=True)\n",
    "\n",
    "df = con.sql(\"\"\"\n",
    "    select\n",
    "        *\n",
    "    from prepared.daily_hydropower_production\n",
    "    where source in (select source from ss_df)\n",
    "\"\"\").to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419037aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This extracts a specific subset of features rather than all the features tsfresh has to offer (which is 100s!)\n",
    "custom_settings = {\n",
    "    \"mean\": None,\n",
    "    \"standard_deviation\": None,\n",
    "    \"kurtosis\": None,\n",
    "    \"skewness\": None,\n",
    "    \"length\": None,\n",
    "    # \"autocorrelation\": [{\"lag\": l} for l in [1, 7, 14, 30, 90]],\n",
    "    \"augmented_dickey_fuller\": [ {\"attr\": \"pvalue\", \"autolag\": \"AIC\"}],\n",
    "}\n",
    "\n",
    "features_df = extract_features(\n",
    "    df,\n",
    "    column_id=\"source\",\n",
    "    column_sort=\"date\",\n",
    "    column_value=\"kwh\",\n",
    "    disable_progressbar=False,\n",
    "    default_fc_parameters=custom_settings\n",
    ")\n",
    "\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad445ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the extracted statistics is easier\n",
    "features_df.plot(kind=\"hist\", y=\"kwh__mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5626faff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I tried a KMeans clustering of the features extracted to look for high level patterns\n",
    "\n",
    "# Ensure your features are scaled (important for distance-based clustering)\n",
    "scaled_features = StandardScaler().fit_transform(features_df.fillna(0))\n",
    "scaled_features_df = pd.DataFrame(scaled_features, columns=features_df.columns)\n",
    "\n",
    "# Store results\n",
    "inertia = []\n",
    "silhouette_scores = []\n",
    "k_range = range(2, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=\"auto\")\n",
    "    labels = kmeans.fit_predict(scaled_features_df.fillna(0))\n",
    "    \n",
    "    inertia.append(kmeans.inertia_)  # Sum of squared distances to centroids\n",
    "    silhouette_scores.append(silhouette_score(scaled_features_df.fillna(0), labels))\n",
    "\n",
    "# Plot results\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Elbow curve\n",
    "ax[0].plot(k_range, inertia, marker='o')\n",
    "ax[0].set_title('Elbow Curve')\n",
    "ax[0].set_xlabel('Number of Clusters')\n",
    "ax[0].set_ylabel('Inertia (Within-Cluster SSE)')\n",
    "\n",
    "# Silhouette scores\n",
    "ax[1].plot(k_range, silhouette_scores, marker='o', color='green')\n",
    "ax[1].set_title('Silhouette Score')\n",
    "ax[1].set_xlabel('Number of Clusters')\n",
    "ax[1].set_ylabel('Silhouette Score (Higher is Better)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2701ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit final KMeans\n",
    "kmeans = KMeans(n_clusters=5, random_state=42, n_init=\"auto\")\n",
    "cluster_labels = kmeans.fit_predict(scaled_features)\n",
    "\n",
    "# Assign cluster labels back to DataFrame\n",
    "scaled_features_df[\"cluster\"] = cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d5a83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features to plot (avoid the cluster column)\n",
    "features_to_plot = [col for col in scaled_features_df.columns if col != \"cluster\"]\n",
    "\n",
    "# Create violin plots for each feature\n",
    "for feature in features_to_plot:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.violinplot(data=scaled_features_df, x=\"cluster\", y=feature, inner=\"quartile\", palette=\"muted\")\n",
    "    plt.title(f\"Distribution of '{feature}' by Cluster\")\n",
    "    plt.xlabel(\"Cluster\")\n",
    "    plt.ylabel(feature)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc1e6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a56d48",
   "metadata": {},
   "source": [
    "## Lessons Learned\n",
    "\n",
    "While I think that the feature extraction and clustering is a good idea, I did not pursue the idea fully. In hindsight:\n",
    "- I should have either abandoned the ADF test completely or used it for a binary split of the data into stationary and non-stationary series.\n",
    "- I could have used features like mean, length and standard deviation to group the series into different training sets for different models.\n",
    "- Given the groupings I could have used the different models to make a powerful ensemble model (like the winner's solution).\n",
    "\n",
    "Overall, I think this method is powerful, but you need to really critically think about what you are looking for when doing this... I was just pursuing it on a whim."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
