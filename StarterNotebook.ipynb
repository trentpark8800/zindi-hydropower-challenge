{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49dgBZKbV5rt",
        "outputId": "5e5c54fe-d688-4d1c-9357-ee58910989ff"
      },
      "source": [
        "## ⚠️ Important Note\n",
        "\n",
        "This is a starter notebook provided by Zindi (the competition site), the original can be found [here](https://zindi.africa/competitions/ibm-skillsbuild-hydropower-climate-optimisation-challenge/data) along with the data used for this challenge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_7qWifJO3j0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCWLFgHoO9tM"
      },
      "outputs": [],
      "source": [
        "all_data_df = pd.read_csv(\"./data/Data.csv\")\n",
        "ss = pd.read_csv(\"./data/SampleSubmission.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ot98ZMpLjmwq",
        "outputId": "cc91e4fd-7c24-4b95-ae61-e1e17196accb"
      },
      "outputs": [],
      "source": [
        "all_data_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ATRF08HZO9xp",
        "outputId": "109e5daa-b9e2-4f8e-819a-b0b89cdb5d50"
      },
      "outputs": [],
      "source": [
        "all_data_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(all_data_df[\"Source\"].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ss.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ss[[\"date\", \"source\"]] = ss[\"ID\"].str.split(\"_\", expand=True, n=1)\n",
        "\n",
        "len(ss[\"source\"].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "AF_gExWoO95e",
        "outputId": "31dc1ea9-a694-4fe0-be64-9b7d43aec5c6"
      },
      "outputs": [],
      "source": [
        "# Split 'Source' into 'consumer_device_X' and 'data_user_Y'\n",
        "all_data_df[['consumer_device', 'data_user']] = all_data_df['Source'].str.extract(r'(consumer_device_\\d+)_data_user_(\\d+)')\n",
        "\n",
        "# Display the updated DataFrame (optional)\n",
        "all_data_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUOD0vgnO9-2"
      },
      "outputs": [],
      "source": [
        "# These are the devices that are not in the test SampleSubmission\n",
        "\n",
        "devices_to_drop = [\"consumer_device_3\",\"consumer_device_5\",\"consumer_device_11\", \"consumer_device_14\",\n",
        "                   \"consumer_device_15\", \"consumer_device_17\", \"consumer_device_24\",\n",
        "                   \"consumer_device_25\",\"consumer_device_27\",\"consumer_device_33\",\"consumer_device_4\",\"consumer_device_9\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "XysRYRCFO-EN",
        "outputId": "334b361e-4a97-41cf-fec8-46c9d6a1efe5"
      },
      "outputs": [],
      "source": [
        "# Filter the DataFrame to include only rows where 'consumer_device' is in the 'devices_to_drop' list.\n",
        "filtered_df = all_data_df[all_data_df['consumer_device'].isin(devices_to_drop)]\n",
        "\n",
        "# Now 'filtered_df' contains only the rows you specified.  You can further process or save this DataFrame.\n",
        "filtered_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "czwL-Mi-O-Jz",
        "outputId": "59000a46-05ab-496b-d760-f727a17a515e"
      },
      "outputs": [],
      "source": [
        "# for all_data[\"Source\"] aggregate by sum on day\n",
        "\n",
        "import pandas as pd\n",
        "# Assuming 'all_data_df' is already defined as in your previous code.\n",
        "# Convert 'Datetime' column to datetime objects if it's not already\n",
        "all_data_df['date_time'] = pd.to_datetime(all_data_df['date_time'])\n",
        "\n",
        "# Extract the date part\n",
        "all_data_df['Date'] = all_data_df['date_time'].dt.date\n",
        "\n",
        "# Group by 'Source' and 'Date', then sum the 'Load' for each group\n",
        "aggregated_data = all_data_df.groupby(['Source', 'Date'])['kwh'].sum().reset_index()\n",
        "\n",
        "# Display the aggregated data\n",
        "aggregated_data.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "id": "XaWOeW9VaLv8",
        "outputId": "ab4e42cc-3675-4e20-89e2-aa6b4d01a008"
      },
      "outputs": [],
      "source": [
        "# Filter data for consumer_device_10\n",
        "consumer_10_data = aggregated_data[aggregated_data['Source'].str.contains('consumer_device_10')]\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "for data_user in consumer_10_data['Source'].unique():\n",
        "    user_data = consumer_10_data[consumer_10_data['Source'] == data_user]\n",
        "    plt.plot(user_data['Date'], user_data['kwh'], label=data_user)\n",
        "\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('kwh')\n",
        "plt.title('kwh Consumption for consumer_device_10 per data_user')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIBO4hvzO-OQ",
        "outputId": "e6dcb5bd-6bf4-4db1-d149-adca6f3f5b91"
      },
      "outputs": [],
      "source": [
        "# Find the minimum and maximum date_time values\n",
        "min_date = aggregated_data['Date'].min()\n",
        "max_date = aggregated_data['Date'].max()\n",
        "\n",
        "print(f\"Minimum date_time: {min_date}\")\n",
        "print(f\"Maximum date_time: {max_date}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8bdrHxLTIl-"
      },
      "outputs": [],
      "source": [
        "# Fill missing date values with 0 kwh\n",
        "\n",
        "# Create a date range\n",
        "date_rng = pd.date_range(start=min_date, end=max_date, freq='D')\n",
        "\n",
        "# Create an empty DataFrame to store the complete data\n",
        "complete_data = pd.DataFrame()\n",
        "\n",
        "# Iterate through each unique 'Source'\n",
        "for source in aggregated_data['Source'].unique():\n",
        "    # Extract data for the current 'Source'\n",
        "    source_data = aggregated_data[aggregated_data['Source'] == source].copy()\n",
        "\n",
        "    # Convert the source data Date to match the type of date_rng\n",
        "    source_data['Date'] = pd.to_datetime(source_data['Date'])\n",
        "\n",
        "    # Create a complete date range for the current 'Source'\n",
        "    source_date_rng = pd.DataFrame({'Date': date_rng})\n",
        "    source_date_rng['Source'] = source\n",
        "\n",
        "    # Merge with the existing data, filling missing 'kwh' values with 0\n",
        "    source_data = pd.merge(source_date_rng, source_data, on=['Date', 'Source'], how='left')\n",
        "    source_data['kwh'] = source_data['kwh'].fillna(0)\n",
        "\n",
        "    # Append to the complete data\n",
        "    complete_data = pd.concat([complete_data, source_data], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "HBH5w_RvUa5l",
        "outputId": "bfac6929-4032-40b3-cb8d-3b6058620a77"
      },
      "outputs": [],
      "source": [
        "complete_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrDUj3ldVO0D"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "\n",
        "# Function to process and forecast per unique consumer_device_x and data_user_y\n",
        "def forecast_arima(all_data, forecast_horizon=30, output_template=None):\n",
        "    # Convert Date column to datetime format\n",
        "    all_data['Date'] = pd.to_datetime(all_data['Date'])\n",
        "\n",
        "    # Extract consumer_device_x and data_user_y\n",
        "    all_data[['consumer_device', 'data_user']] = all_data['Source'].str.extract(r'consumer_device_(\\d+)_data_user_(\\d+)')\n",
        "\n",
        "    # Ensure data is sorted by consumer_device, data_user, and Date\n",
        "    all_data = all_data.sort_values(by=['consumer_device', 'data_user', 'Date'])\n",
        "\n",
        "    # Store forecasts\n",
        "    forecast_results = []\n",
        "\n",
        "    # Process each unique consumer_device_x and data_user_y combination\n",
        "    for (consumer_device, data_user), group in all_data.groupby([\"consumer_device\", \"data_user\"]):\n",
        "        # Set Date as index\n",
        "        group = group.set_index(\"Date\")\n",
        "\n",
        "        # Ensure data is in the correct format\n",
        "        group = group.asfreq('D').fillna(method='ffill')  # Fill missing dates with last known value\n",
        "\n",
        "        # Fit ARIMA model\n",
        "        try:\n",
        "            model = ARIMA(group[\"kwh\"], order=(5, 1, 0))  # ARIMA(5,1,0) as a baseline\n",
        "            fitted_model = model.fit()\n",
        "\n",
        "            # Forecast for the next forecast_horizon days\n",
        "            forecast_dates = pd.date_range(start=group.index[-1] + pd.Timedelta(days=1),\n",
        "                                           periods=forecast_horizon, freq='D')\n",
        "            forecast_values = fitted_model.forecast(steps=forecast_horizon)\n",
        "\n",
        "            # Store results in required format\n",
        "            forecast_df = pd.DataFrame({\n",
        "                \"ID\": [f\"{date.strftime('%Y-%m-%d')}_consumer_device_{consumer_device}_data_user_{data_user}\"\n",
        "                        for date in forecast_dates],\n",
        "                \"kwh\": forecast_values\n",
        "            })\n",
        "\n",
        "            forecast_results.append(forecast_df)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {consumer_device}_{data_user}: {e}\")\n",
        "\n",
        "    # Combine all forecasts into a single DataFrame\n",
        "    forecast_df = pd.concat(forecast_results, ignore_index=True)\n",
        "\n",
        "    # If an output template is provided, align the output format\n",
        "    if output_template is not None:\n",
        "        output_template = output_template.drop(columns=['kwh'], errors='ignore')\n",
        "        final_output = output_template.merge(forecast_df, on='ID', how='left').fillna(0)\n",
        "    else:\n",
        "        final_output = forecast_df\n",
        "\n",
        "    return final_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTs2lW0kVZ2X"
      },
      "outputs": [],
      "source": [
        "forecast = forecast_arima(all_data=complete_data, forecast_horizon=30, output_template=ss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "XYGhOeWvds-3",
        "outputId": "fcfe82e0-867b-4ec3-dc03-912d190ca968"
      },
      "outputs": [],
      "source": [
        "forecast.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YiLyLAOjUzz"
      },
      "outputs": [],
      "source": [
        "# prompt: does forecast[\"kwh\"] contain nans if so replace with 0\n",
        "\n",
        "# Check for NaN values in the 'kwh' column and replace them with 0\n",
        "forecast[\"kwh\"] = forecast[\"kwh\"].fillna(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snlyo5EwdtEg",
        "outputId": "48d67fa7-4279-4490-c417-e0d90bd628b5"
      },
      "outputs": [],
      "source": [
        "len(complete_data), len(forecast), len(ss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmHiFLANdtII"
      },
      "outputs": [],
      "source": [
        "forecast[[\"ID\", \"kwh\"]].to_csv(\"./submissions/starter_notebook_forecast.csv\", index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9E7unLagy4f",
        "outputId": "ef749d19-6a7b-44be-e403-ee8273c65f04"
      },
      "outputs": [],
      "source": [
        "# prompt: list the difference in the ID between forecast and ss\n",
        "\n",
        "# Assuming 'forecast' and 'ss' DataFrames are already defined as in your provided code.\n",
        "\n",
        "# Convert 'ID' columns to sets for efficient comparison\n",
        "forecast_ids = set(forecast['ID'])\n",
        "ss_ids = set(ss['ID'])\n",
        "\n",
        "# Find IDs present in forecast but not in ss\n",
        "forecast_only_ids = forecast_ids - ss_ids\n",
        "\n",
        "# Find IDs present in ss but not in forecast\n",
        "ss_only_ids = ss_ids - forecast_ids\n",
        "\n",
        "# Print the IDs that are in forecast but not in ss\n",
        "print(\"IDs in 'forecast' but not in 'ss':\")\n",
        "print(forecast_only_ids)\n",
        "\n",
        "\n",
        "# Print the IDs that are in ss but not in forecast\n",
        "print(\"\\nIDs in 'ss' but not in 'forecast':\")\n",
        "print(ss_only_ids)\n",
        "\n",
        "# Print the number of IDs that differ\n",
        "print(f\"\\nNumber of IDs that differ: {len(forecast_only_ids) + len(ss_only_ids)}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
