{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9e93af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import duckdb as ddb\n",
    "import optuna\n",
    "import numpy as np\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.models import LinearRegressionModel\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.metrics import rmse\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004e27b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = ddb.connect(\"./kalam_hydropower.db\", read_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f61814b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are devices with weak signals which we can exclude from the analysis - these were mentioned in the samplesubmission notebook\n",
    "devices_to_drop = [\"3\", \"5\", \"11\", \"14\", \"15\", \"17\", \"24\", \"25\", \"27\", \"33\", \"4\", \"9\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b285993e",
   "metadata": {},
   "source": [
    "## Modelling and Submission\n",
    "\n",
    "Now that we have an idea of features and reasonable data structures, this next section aims to:\n",
    "- Structure the data to be used with the Darts package - this depends on a `TimeSeries` data type rather than dataframes\n",
    "- Splitting and scaling the data appropriately to avoid leakage in our test sets\n",
    "- Creating an appropriate scoring and prediction function to easily format our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be53834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is clear from our sub-plots that there is a long period of 'no signal' in the data, so I'll use data from 2024-07-01 onward\n",
    "# for simplicity and to avoid too much zero biasing. Ofcourse, the fillna(0) adds some degree of biasing as well\n",
    "filtered_to_forecast_df = con.sql(\n",
    "    \"\"\"select\n",
    "        date, source, consumer_device, data_user, kwh \n",
    "    from prepared.daily_hydropower_production\n",
    "\"\"\").to_df()\n",
    "\n",
    "filtered_to_forecast_df = filtered_to_forecast_df[~(filtered_to_forecast_df[\"consumer_device\"].isin(devices_to_drop))]\n",
    "\n",
    "filtered_to_forecast_pivotted_df = filtered_to_forecast_df.pivot(columns=\"source\", index=\"date\", values=\"kwh\").copy()\n",
    "filtered_to_forecast_pivotted_df.fillna(0, inplace=True)\n",
    "\n",
    "filtered_to_forecast_df = filtered_to_forecast_pivotted_df.melt(value_vars=list(filtered_to_forecast_pivotted_df.columns), var_name=\"source\", value_name=\"kwh\", ignore_index=False)\n",
    "\n",
    "filtered_to_forecast_df.reset_index(inplace=True, drop=False)\n",
    "\n",
    "\n",
    "filtered_to_forecast_df = filtered_to_forecast_df[filtered_to_forecast_df[\"date\"] >= pd.Timestamp(\"2024-07-01\")]\n",
    "\n",
    "filtered_to_forecast_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1406741",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_to_forecast_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb005db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I acutally want to use the device, user and source as static covariates in darts, this is a bit of a roundabout\n",
    "# way of getting to the values but this gets the consumer device and data user numbers back\n",
    "filtered_to_forecast_df[['consumer_device', 'data_user']] = filtered_to_forecast_df['source'].str.extract(r'consumer_device_(\\d+)_data_user_(\\d+)')\n",
    "filtered_to_forecast_df[\"consumer_device\"] = filtered_to_forecast_df[\"consumer_device\"].astype(int)\n",
    "filtered_to_forecast_df[\"data_user\"] = filtered_to_forecast_df[\"data_user\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd7b339",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_to_forecast_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941ce43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = con.sql(\"select * from prepared.daily_features\").to_df()\n",
    "\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145d46b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_ts = TimeSeries.from_group_dataframe(\n",
    "#     filtered_to_forecast_df,\n",
    "#     time_col=\"date\",\n",
    "#     group_cols=\"source\",\n",
    "#     static_cols=[\"consumer_device\", \"data_user\"],\n",
    "#     value_cols=[\"kwh\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9607de5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_ts[0].static_covariates_values()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bf01c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I setup a dictionary with a key per series, and a value containing the darts timeseries' incl. training, testing and scaled data\n",
    "# as well as the individual scalers used to invert the transformation\n",
    "ts_dict = {}\n",
    "\n",
    "for series_name in filtered_to_forecast_df[\"source\"].unique():\n",
    "        \n",
    "    try:\n",
    "\n",
    "        series_ts = TimeSeries.from_dataframe(\n",
    "            df=filtered_to_forecast_df[filtered_to_forecast_df[\"source\"] == series_name],\n",
    "            time_col=\"date\",\n",
    "            value_cols=[\"kwh\"]\n",
    "        )\n",
    "\n",
    "        scaler = Scaler(StandardScaler())\n",
    "\n",
    "        series_scaled_ts = scaler.fit_transform(series_ts)\n",
    "\n",
    "        train_ts, test_ts = series_ts.split_after(len(series_ts) - 31)\n",
    "\n",
    "        train_scaler = Scaler(StandardScaler())\n",
    "\n",
    "        train_scaled_ts = train_scaler.fit_transform(train_ts)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process series {series_name} due to {e}\")\n",
    "        continue\n",
    "\n",
    "    ts_dict[series_name] = {\n",
    "        \"ts\": series_ts,\n",
    "        \"scaled_ts\": series_scaled_ts,\n",
    "        \"scaler\": scaler,\n",
    "        \"train_ts\": train_ts,\n",
    "        \"train_scaled_ts\": train_scaled_ts,\n",
    "        \"train_scaler\": train_scaler,\n",
    "        \"test_ts\": test_ts,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d9b0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In darts you can pass a list of timeseries to a model's .fit function so that the model is trained on multiple series at once\n",
    "train_ts_list = [ts_dict[series_name][\"train_scaled_ts\"] for series_name in ts_dict.keys()]\n",
    "\n",
    "# These are the full series which we will use to train the final model\n",
    "ts_list = [ts_dict[series_name][\"scaled_ts\"] for series_name in ts_dict.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617eb367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The features we want to use need to also be scaled so that they can be used with a model like LinearRegression\n",
    "features_ts = TimeSeries.from_dataframe(features_df, time_col=\"date\")\n",
    "\n",
    "features_scaler = Scaler(StandardScaler())\n",
    "\n",
    "scaled_features_ts = features_scaler.fit_transform(features_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de87201d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need the sample submission file to produce an 'accurate score' i.e. how well am I doing on the series that my model will\n",
    "# actually be validated on\n",
    "ss_df = pd.read_csv(\"./data/SampleSubmission.csv\")\n",
    "ss_df[[\"date\", \"source\"]] = ss_df[\"ID\"].str.split(\"_\", expand=True, n=1)\n",
    "\n",
    "ss_df[\"date\"] = pd.to_datetime(ss_df[\"date\"])\n",
    "\n",
    "ss_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed99f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_model(model, ts_dict, ss_df, future_covs=None, forecast_horizon=31, scaled=False):\n",
    "    \"\"\"A utility function to score a model based on the data in the submission set\"\"\"\n",
    "\n",
    "    rmse_scores = []\n",
    "\n",
    "    series_to_forecast = set(ts_dict.keys()).intersection(set(ss_df[\"source\"].unique()))\n",
    "\n",
    "    for index, series_name in enumerate(series_to_forecast):\n",
    "\n",
    "        predictions = model.predict(forecast_horizon, series=ts_dict[series_name][\"train_scaled_ts\"], future_covariates=future_covs[index], show_warnings=False)\n",
    "\n",
    "        if scaled:\n",
    "            predictions = ts_dict[series_name][\"train_scaler\"].inverse_transform(predictions)\n",
    "        \n",
    "        rmse_scores.append(rmse(ts_dict[series_name][\"test_ts\"], predictions))\n",
    "    \n",
    "    return np.mean(rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10d7378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_predictions(model, ts_dict, ss_df, future_covs, forecast_horizon=31, scaled=False):\n",
    "    \"\"\"A utility function which can easily create a submission based on the sample submission file\"\"\"\n",
    "\n",
    "    predictions_df = pd.DataFrame()\n",
    "\n",
    "    series_to_forecast = set(ts_dict.keys()).intersection(set(ss_df[\"source\"].unique()))\n",
    "\n",
    "    for index, series_name in enumerate(series_to_forecast):\n",
    "\n",
    "        predictions = model.predict(forecast_horizon, series=ts_dict[series_name][\"scaled_ts\"], future_covariates=future_covs[index])\n",
    "\n",
    "        if scaled:\n",
    "            predictions = ts_dict[series_name][\"scaler\"].inverse_transform(predictions)\n",
    "\n",
    "        pred_df = predictions.to_dataframe()\n",
    "        pred_df[\"source\"] = series_name\n",
    "\n",
    "        predictions_df = pd.concat([predictions_df, pred_df])\n",
    "\n",
    "\n",
    "        predictions_df.sort_values(by=[\"source\", \"date\"], inplace=True)\n",
    "\n",
    "        predictions_df[\"ID\"] = predictions_df.index.astype(str) + \"_\" + predictions_df[\"source\"]\n",
    "\n",
    "    return predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8dc325",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {\n",
    "    \"lags\": [-1],\n",
    "    \"output_chunk_length\": 9,\n",
    "    \"lags_future_covariates\": [-30],\n",
    "    \"use_static_covariates\": False,\n",
    "    \"random_state\": 42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0c27de",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting up my model\n",
    "linear_model = LinearRegressionModel(\n",
    "    **model_kwargs\n",
    ")\n",
    "\n",
    "future_covs = [scaled_features_ts[['precip_snow_ratio']] for _ in range(len(train_ts_list))]\n",
    "\n",
    "linear_model.fit(train_ts_list, future_covariates=future_covs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a78747",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_model(linear_model, ts_dict, ss_df, future_covs, scaled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133d971d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Full model - now we can use all the data available to us to make the best model possible\n",
    "full_model = LinearRegressionModel(\n",
    "    **model_kwargs\n",
    ")\n",
    "\n",
    "future_covs = [scaled_features_ts[['precip_snow_ratio']] for _ in range(len(ts_list))]\n",
    "\n",
    "full_model.fit(ts_list, future_covariates=future_covs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f240cfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_df = create_predictions(full_model, ts_dict, ss_df, future_covs, scaled=True)\n",
    "\n",
    "forecast_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c839b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_df[[\"ID\",\"kwh\"]].to_csv(\"./submissions/my_forecast.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3d675b",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6e1f39",
   "metadata": {},
   "source": [
    "# But Wait, How Did You Get Those Parameters?\n",
    "\n",
    "To find the optimal parameters for my LinearRegressor I leveraged `optuna` to test many variants of parameter combinations. Then tried using the top X results for submissions on Zindi to find the best validation score I could."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98291c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def covariate_objective(trial: optuna.Trial):\n",
    "    lags = trial.suggest_categorical(\"lags\", [[-1], [-1, -7], [-1, -14]])\n",
    "    output_chunk_length = trial.suggest_int(\"output_chunk_length\", 5, 20)\n",
    "    future_cov_lags = trial.suggest_categorical(\"future_cov_lags\", [[0], [-30], [-60], [-90]])\n",
    "\n",
    "    future_cov_options = [\n",
    "        [\"avg_temperature\"],\n",
    "        [\"avg_dewpoint_temperature\"],\n",
    "        [\"wind_speed\"],\n",
    "        [\"precip_snow_ratio\"],\n",
    "        [\"dayofyear\"],\n",
    "        [\"avg_temperature\", \"avg_dewpoint_temperature\", \"wind_speed\", \"precip_snow_ratio\", \"dayofyear\"],\n",
    "    ]\n",
    "    selected_covariates = trial.suggest_categorical(\"future_covs\", future_cov_options)\n",
    "    \n",
    "    future_covs = [scaled_features_ts[selected_covariates]] * len(train_ts_list)\n",
    "\n",
    "    # Create and fit model\n",
    "    linear_model = LinearRegressionModel(\n",
    "        lags=lags,\n",
    "        output_chunk_length=output_chunk_length,\n",
    "        use_static_covariates=False, ## In my original code I tried out using the consumer devices and users as static covs\n",
    "        lags_future_covariates=future_cov_lags,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    linear_model.fit(train_ts_list, future_covariates=future_covs)\n",
    "\n",
    "    mean_rmse = score_model(linear_model, ts_dict, ss_df, future_covs=future_covs, scaled=True)\n",
    "\n",
    "    return mean_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1c2589",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "\n",
    "study.optimize(covariate_objective, n_trials=50, n_jobs=8, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7375e5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
